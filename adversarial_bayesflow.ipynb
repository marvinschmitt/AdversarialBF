{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Adversarial BayesFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from scipy import stats\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('../BayesFlow')))\n",
    "\n",
    "from bayesflow.trainers import ParameterEstimationTrainer\n",
    "from bayesflow.networks import InvertibleNetwork, InvariantNetwork\n",
    "from bayesflow.amortizers import SingleModelAmortizer\n",
    "from bayesflow.models import GenerativeModel\n",
    "from bayesflow.diagnostics import true_vs_estimated\n",
    "from bayesflow.exceptions import ConfigurationError\n",
    "\n",
    "from bayesflow.applications.priors import GaussianMeanPrior, TPrior, GaussianMeanCovPrior\n",
    "from bayesflow.applications.simulators import GaussianMeanSimulator, MultivariateTSimulator, GaussianMeanCovSimulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_latent_space(z, log_det_J):\n",
    "    \"\"\" Computes the Kullback-Leibler divergence (Maximum Likelihood Loss) between true and approximate\n",
    "    posterior using simulated data and parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    loss = tf.reduce_mean(0.5 * tf.square(tf.norm(z, axis=-1)) - log_det_J)\n",
    "    return loss\n",
    "\n",
    "def maximum_mean_discrepancy(source_samples, target_samples, minimum=0.):\n",
    "    \"\"\" This Maximum Mean Discrepancy (MMD) loss is calculated with a number of different Gaussian kernels.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sigmas = [\n",
    "        1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 5, 10, 15, 20, 25, 30, 35, 100,\n",
    "        1e3, 1e4, 1e5, 1e6\n",
    "    ]\n",
    "    gaussian_kernel = partial(_gaussian_kernel_matrix, sigmas=sigmas)\n",
    "    loss_value = _mmd_kernel(source_samples, target_samples, kernel=gaussian_kernel)\n",
    "    loss_value = tf.maximum(minimum, loss_value) \n",
    "    return loss_value\n",
    "\n",
    "def _gaussian_kernel_matrix(x, y, sigmas):\n",
    "    \"\"\" Computes a Gaussian Radial Basis Kernel between the samples of x and y.\n",
    "\n",
    "    We create a sum of multiple gaussian kernels each having a width :math:`\\sigma_i`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x :  tf.Tensor of shape (M, num_features)\n",
    "    y :  tf.Tensor of shape (N, num_features)\n",
    "    sigmas : list(float)\n",
    "        List which denotes the widths of each of the gaussians in the kernel.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    kernel: tf.Tensor\n",
    "        RBF kernel of shape [num_samples{x}, num_samples{y}]\n",
    "    \"\"\"\n",
    "    def norm(v):\n",
    "        return tf.reduce_sum(tf.square(v), 1)\n",
    "    beta = 1. / (2. * (tf.expand_dims(sigmas, 1)))\n",
    "    dist = tf.transpose(norm(tf.expand_dims(x, 2) - tf.transpose(y)))\n",
    "    s = tf.matmul(beta, tf.reshape(dist, (1, -1)))\n",
    "    kernel = tf.reshape(tf.reduce_sum(tf.exp(-s), 0), tf.shape(dist))\n",
    "    return kernel\n",
    "\n",
    "def _mmd_kernel(x, y, kernel=_gaussian_kernel_matrix):\n",
    "    \"\"\" Computes the Maximum Mean Discrepancy (MMD) of two samples: x and y.\n",
    "\n",
    "    Maximum Mean Discrepancy (MMD) is a distance-measure between the samples of the distributions of x and y.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x      : tf.Tensor of shape (num_samples, num_features)\n",
    "    y      : tf.Tensor of shape (num_samples, num_features)\n",
    "    kernel : callable, default: _gaussian_kernel_matrix\n",
    "        A function which computes the kernel in MMD.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : tf.Tensor\n",
    "        squared maximum mean discrepancy loss, shape (,)\n",
    "    \"\"\"\n",
    "\n",
    "    loss = tf.reduce_mean(kernel(x, x))  # lint error: sigmas unfilled\n",
    "    loss += tf.reduce_mean(kernel(y, y))  # lint error: sigmas unfilled\n",
    "    loss -= 2 * tf.reduce_mean(kernel(x, y))  # lint error: sigmas unfilled\n",
    "    return loss\n",
    "\n",
    "def mmd_kl_loss(network, *args, mmd_weight=1.0):\n",
    "    \"\"\"KL loss in latent z space, MMD loss in summary space.\"\"\"\n",
    "    \n",
    "    # Apply net and unpack \n",
    "    x_sum, out = network(*args, return_summary=True)\n",
    "    z, log_det_J = out\n",
    "    \n",
    "    # Apply MMD loss to x_sum\n",
    "    z_normal = tf.random.normal(x_sum.shape)\n",
    "    mmd_loss = maximum_mean_discrepancy(x_sum, z_normal)\n",
    "    \n",
    "    # Apply KL loss for inference net\n",
    "    kl_loss = kl_latent_space(z, log_det_J)\n",
    "    \n",
    "    # Sum and return losses\n",
    "    return kl_loss + mmd_weight * mmd_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) MVN means\n",
    "**Task:** Learn means of a 5-variate Gaussian with unit variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define minimalistic BayesFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 5\n",
    "\n",
    "#########\n",
    "\n",
    "prior = GaussianMeanPrior(D=D)\n",
    "simulator = GaussianMeanSimulator(D=D)\n",
    "generative_model = GenerativeModel(prior, simulator)\n",
    "\n",
    "#########\n",
    "\n",
    "summary_meta = {\n",
    "    'n_dense_s1': 2,\n",
    "    'n_dense_s2': 2,\n",
    "    'n_dense_s3': 2,\n",
    "    'n_equiv':    1,\n",
    "    'dense_s1_args': {'activation': 'relu', 'units': 32},\n",
    "    'dense_s2_args': {'activation': 'relu', 'units': 32},\n",
    "    'dense_s3_args': {'activation': 'relu', 'units': 32},\n",
    "}\n",
    "\n",
    "class BottleneckSummaryNet(tf.keras.Model):\n",
    "    def __init__(self, inv_meta={}, n_out=10, activation_out=None):\n",
    "        super(BottleneckSummaryNet, self).__init__()\n",
    "\n",
    "        self.invariant_net = InvariantNetwork(inv_meta)\n",
    "        self.out_layer = tf.keras.layers.Dense(n_out, activation=activation_out)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        out_inv = self.invariant_net(x)\n",
    "        out = self.out_layer(out_inv)\n",
    "        return out\n",
    "\n",
    "\n",
    "summary_net = BottleneckSummaryNet(inv_meta=summary_meta, \n",
    "                                   n_out=D*2,  # one mean and variance per dim\n",
    "                                   activation_out=None  # linear\n",
    "                                  )\n",
    "\n",
    "\n",
    "inference_meta = {\n",
    "    'n_coupling_layers': 2,\n",
    "    's_args': {\n",
    "        'units': [32, 32, 32],\n",
    "        'activation': 'elu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "    },\n",
    "    't_args': {\n",
    "        'units': [32, 32, 32],\n",
    "        'activation': 'elu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "    },\n",
    "    'n_params': D,\n",
    "    'alpha': 1.9,\n",
    "    'permute': True\n",
    "}\n",
    "\n",
    "inference_net = InvertibleNetwork(inference_meta)\n",
    "\n",
    "amortizer = SingleModelAmortizer(inference_net, summary_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing networks from scratch.\n"
     ]
    }
   ],
   "source": [
    "trainer = ParameterEstimationTrainer(amortizer,\n",
    "                      generative_model,\n",
    "                      loss=mmd_kl_loss,\n",
    "                      learning_rate=0.0001,\n",
    "                      checkpoint_path='export_ckpt/stefan/means_5D',\n",
    "                      max_to_keep = 2\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converge BayesFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating initial 20000 data sets...\n",
      "Converting 20000 simulations to a TensorFlow data set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172b20f952f94bedbd433d706f8b8411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 1:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b900230347fd43bab02d934ee53e6929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 2:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating new 20000 data sets and appending to previous...\n",
      "New total number of simulated data sets: 40000\n",
      "Converting 40000 simulations to a TensorFlow data set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712327cd8f4f4c7f8a36f65ed5a3c82b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 1:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = trainer.train_rounds(epochs=2, rounds=2, sim_per_round=20000, batch_size=1024, n_obs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate (quick and dirty)\n",
    "p, x = trainer._forward_inference(200, 100)\n",
    "param_samples = trainer.network.sample(x, n_samples=200)\n",
    "param_means = param_samples.mean(axis=0)\n",
    "true_vs_estimated(p, param_means, ['mu{}'.format(i) for i in range(1, 5+1)], figsize=(20,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.array(trainer.network.summary_net(x))\n",
    "sns.pairplot(pd.DataFrame(s, columns=['s_{}'.format(i) for i in range(1, s.shape[1]+1)]), kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypersetup for all tasks\n",
    "D = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_analytic_posterior(prior, simulator, x):\n",
    "    n_sim, n_obs, D = x.shape\n",
    "    \n",
    "    # Set up variables\n",
    "    x_bar = np.mean(x, axis=1)                 # empirical mean\n",
    "    sigma_0 = np.eye(D) * prior.mu_scale       # mu prior covariance\n",
    "    sigma_0_inv = np.linalg.inv(sigma_0)       # inverse mu prior covariance\n",
    "    mu_0 = np.ones((D, 1)) * prior.mu_mean     # mu prior mean\n",
    "    sigma = simulator.sigma                    # likelihood covariance\n",
    "    sigma_inv = np.linalg.inv(sigma)           # inverse likelihood covariance\n",
    "    \n",
    "    mu_posterior_covariance = np.stack([np.linalg.inv(sigma_0_inv + n_obs*sigma_inv)] * n_sim)\n",
    "    \n",
    "    mu_posterior_mean = mu_posterior_covariance @ (sigma_0_inv @ mu_0 + n_obs * (sigma_inv @ x_bar[..., np.newaxis]))   \n",
    "    mu_posterior_mean = mu_posterior_mean.reshape(n_sim, D)\n",
    "\n",
    "    return mu_posterior_mean, mu_posterior_covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_diagnostics(trainer, generative_model, theta=None, x=None, print_pairplot_summarynet=False, print_pairplot_posteriors=False):\n",
    "    theta, x = generative_model(200, 100) if theta is None and x is None else (theta, x)\n",
    "    param_samples = trainer.network.sample(x, n_samples=200)\n",
    "    param_means = param_samples.mean(axis=0)\n",
    "    \n",
    "    # true parameters\n",
    "    print(\"BayesFlow (x) vs. true thetas (y) -- Recovery of true thetas\")\n",
    "    true_vs_estimated(theta, param_means, ['mu{}'.format(i) for i in range(1, 5+1)], figsize=(20,4))\n",
    "    \n",
    "    # analytic posteriors\n",
    "    print(\"\\n\\nBayesFlow (x) vs. analytic posterior means (y) -- Recovery of analytic posterior means\")\n",
    "    prior = trainer.generative_model.prior.__self__\n",
    "    simulator = trainer.generative_model.simulator\n",
    "    posterior_means, posterior_covariances = calculate_analytic_posterior(prior, simulator, x)\n",
    "    posterior_variances = posterior_covariances.diagonal(axis1=1, axis2=2)\n",
    "    true_vs_estimated(posterior_means, param_means, ['mu{}'.format(i) for i in range(1, 5+1)], figsize=(20,4))\n",
    "    \n",
    "    print(\"\\n\\nAnalytic posterior means (x) vs. true thetas (y)\")\n",
    "    true_vs_estimated(posterior_means, theta, ['mu{}'.format(i) for i in range(1, 5+1)], figsize=(20,4))\n",
    "\n",
    "    if print_pairplot_summarynet:\n",
    "        print('\\n\\nSummary network response for one batch')\n",
    "        s = np.array(trainer.network.summary_net(x))\n",
    "        sns.pairplot(pd.DataFrame(s, columns=['s_{}'.format(i) for i in range(1, s.shape[1]+1)]), kind=\"kde\")\n",
    "        \n",
    "    if print_pairplot_posteriors:\n",
    "        print('\\n\\nFull posterior for one dataset.')\n",
    "        p = param_samples[0]\n",
    "        sns.pairplot(pd.DataFrame(p, columns=['dim_{}'.format(i) for i in range(1, p.shape[1]+1)]), kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A1) Wrong Prior\n",
    "The prior over the multivariate Gaussian's means is Gaussian: $\\mu\\sim\\mathcal{N}(\\mu_\\mu, \\sigma_\\mu)$\n",
    "\n",
    "During training, the mean's prior was $\\mu\\sim\\mathcal{N}(0, 1)$. This adversarial tasks varies the prior in three steps for the evaluation:\n",
    "\n",
    "- **(A1)-1** Wrong (free) prior location: $\\mu\\sim\\mathcal{N}(\\mu_\\mu, 1)$\n",
    "- **(A1)-2** Wrong (free) prior scale: $\\mu\\sim\\mathcal{N}(0, \\sigma_\\mu)$\n",
    "- **(A1)-3** Wrong (free) prior location and scale: $\\mu\\sim\\mathcal{N}(\\mu_\\mu, \\sigma_\\mu)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A1)-1 Wrong prior location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior wrong, model misspecified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prior = GaussianMeanPrior(D=D, mu_mean=5, mu_scale=1)\n",
    "simulator = GaussianMeanSimulator(D=D)\n",
    "generative_model = GenerativeModel(prior, simulator)\n",
    "\n",
    "adversarial_diagnostics(trainer, generative_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A1)-2 Wrong prior scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prior = GaussianMeanPrior(D=D, mu_mean=0, mu_scale=5)\n",
    "simulator = GaussianMeanSimulator(D=D)\n",
    "generative_model = GenerativeModel(prior, simulator)\n",
    "\n",
    "adversarial_diagnostics(trainer, generative_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A1)-3 Wrong prior location and scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prior = GaussianMeanPrior(D=D, mu_mean=5, mu_scale=5)\n",
    "simulator = GaussianMeanSimulator(D=D)\n",
    "generative_model = GenerativeModel(prior, simulator)\n",
    "\n",
    "adversarial_diagnostics(trainer, generative_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A2) Wrong Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = GaussianMeanPrior(D=D, mu_mean=0, mu_scale=1)\n",
    "simulator = GaussianMeanSimulator(D=D, s = 5.0)\n",
    "generative_model = GenerativeModel(prior, simulator)\n",
    "\n",
    "adversarial_diagnostics(trainer, generative_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = GaussianMeanPrior(D=D, mu_mean=0, mu_scale=1)\n",
    "simulator = GaussianMeanSimulator(D=D, s = [1, 1, 10, 5, 3])\n",
    "generative_model = GenerativeModel(prior, simulator)\n",
    "\n",
    "adversarial_diagnostics(trainer, generative_model, print_pairplot_summarynet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrong likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = GaussianMeanPrior(D=D, mu_mean=0, mu_scale=1)\n",
    "simulator = MultivariateTSimulator(df=2)\n",
    "\n",
    "means = prior(200)\n",
    "sigma = np.ones_like(means)\n",
    "theta = np.concatenate((means, sigma), axis=1)\n",
    "x = simulator(theta, 100)\n",
    "\n",
    "\n",
    "adversarial_diagnostics(trainer, generative_model=None, theta=theta, x=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A4) Contamination\n",
    "$\\mathbf{x}_n = \\mathbf{x}_n + \\xi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    n_sim, n_obs, data_dim = x.shape\n",
    "    s = np.std(x, axis=1)\n",
    "    s_reshaped = s.reshape(n_sim, 1, data_dim).repeat(n_obs, axis=1)\n",
    "    x_normalized = np.divide(x, s_reshaped)\n",
    "    return x_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pink noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, the contamination $\\xi$ is *pink noise* (aka $\\frac{1}{f}$ noise) and added to the data $x$. The contaminated data is then normalized to obtain unit variance again.\n",
    "\n",
    "$\\tilde{x}=\\dfrac{x+\\xi}{\\sigma_{x+\\xi}}$ with $\\xi \\sim \\frac{1}{f}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior correct, model misspecified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorednoise as cn\n",
    "\n",
    "prior = GaussianMeanPrior(D=D, mu_mean=0, mu_scale=1)\n",
    "simulator = GaussianMeanSimulator(D=D)\n",
    "\n",
    "generative_model = GenerativeModel(prior, simulator)\n",
    "theta, x = generative_model(200, 100)\n",
    "\n",
    "lamda = 1.0  # contribution of contamination xi \n",
    "\n",
    "xi = cn.powerlaw_psd_gaussian(exponent=1, size=x.shape)\n",
    "\n",
    "x_tilde = normalize(x + lamda * xi)\n",
    "\n",
    "adversarial_diagnostics(trainer, generative_model=None, theta=theta, x=x_tilde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t Noise\n",
    "In this scenario, the contamination $\\xi$ is $t-$distributed and added to the data $x$. The contaminated data is then normalized to obtain unit variance again.\n",
    "\n",
    "$\\tilde{x}=\\dfrac{x+\\xi}{\\sigma_{x+\\xi}}$ with $\\xi \\sim t_2(0, \\mathbb{I})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = GaussianMeanPrior(D=D, mu_mean=0, mu_scale=1)\n",
    "simulator = GaussianMeanSimulator(D=D)\n",
    "generative_model = GenerativeModel(prior, simulator)\n",
    "\n",
    "theta, x = generative_model(200, 100)\n",
    "\n",
    "n_sim, n_obs, data_dim = x.shape\n",
    "\n",
    "\n",
    "lamda = 0.5\n",
    "xi_theta = np.concatenate((np.zeros((n_sim, data_dim)), np.ones((n_sim, data_dim))), axis=1)\n",
    "xi_simulator = MultivariateTSimulator(df=2)\n",
    "xi = xi_simulator(xi_theta, n_obs)\n",
    "\n",
    "x_tilde = normalize(x + lamda * xi)\n",
    "\n",
    "adversarial_diagnostics(trainer, generative_model=None, theta=theta, x=x_tilde, print_pairplot_posteriors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) MVN means and full covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 5\n",
    "\n",
    "#########\n",
    "\n",
    "class NormalInverseWishartPrior:\n",
    "    def __init__(self, D, mu_0=0.0, lamda_0=1.0, Psi_0=None, nu_0=None):\n",
    "        if Psi_0 is None:\n",
    "            Psi_0 = np.eye(D)\n",
    "        if nu_0 is None:\n",
    "            nu_0 = D+1\n",
    "        self.D = D,\n",
    "        self.mu_0 = mu_0\n",
    "        self.lamda_0 = lamda_0\n",
    "        self.Psi_0 = Psi_0\n",
    "        self.nu_0 = nu_0\n",
    "        \n",
    "        self.cov_prior = stats.invwishart(self.nu_0, self.Psi_0)\n",
    "        \n",
    "    def __call__(self, n_sim):\n",
    "        cov = self.cov_prior.rvs(n_sim)\n",
    "        tril_cov = tf.linalg.cholesky((1.0 / self.lamda_0) * cov)\n",
    "        means = tfp.distributions.MultivariateNormalTriL(self.mu_0, tril_cov).sample()\n",
    "        \n",
    "        return np.array(means, dtype=np.float32), np.array(cov, dtype=np.float32)\n",
    "\n",
    "\n",
    "def param_transform_full_cov(theta):\n",
    "    means, cov = theta\n",
    "    means = np.array(means)\n",
    "    n_sim, D = means.shape\n",
    "    cov = np.array(cov)\n",
    "    cov = cov[np.tril(cov).nonzero()].reshape(n_sim, -1)\n",
    "    return np.concatenate([means, cov], axis=1)\n",
    "\n",
    "\n",
    "mu_0 = 0.0\n",
    "lamda_0 = 1.0 \n",
    "Psi_0 = np.eye(D)\n",
    "nu_0 = D+1\n",
    "prior = NormalInverseWishartPrior(D=D, mu_0=mu_0, lamda_0=lamda_0, Psi_0=Psi_0, nu_0=nu_0)\n",
    "simulator = GaussianMeanCovSimulator()\n",
    "generative_model = GenerativeModel(prior, simulator, param_transform=param_transform_full_cov)\n",
    "\n",
    "#########\n",
    "\n",
    "summary_meta = {\n",
    "    'n_dense_s1': 2,\n",
    "    'n_dense_s2': 2,\n",
    "    'n_dense_s3': 2,\n",
    "    'n_equiv':    2,\n",
    "    'dense_s1_args': {'activation': 'relu', 'units': 32},\n",
    "    'dense_s2_args': {'activation': 'relu', 'units': 32},\n",
    "    'dense_s3_args': {'activation': 'relu', 'units': 32},\n",
    "}\n",
    "\n",
    "class BottleneckSummaryNet(tf.keras.Model):\n",
    "    def __init__(self, inv_meta={}, n_out=10, activation_out=None):\n",
    "        super(BottleneckSummaryNet, self).__init__()\n",
    "\n",
    "        self.invariant_net = InvariantNetwork(inv_meta)\n",
    "        self.out_layer = tf.keras.layers.Dense(n_out, activation=activation_out)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        out_inv = self.invariant_net(x)\n",
    "        out = self.out_layer(out_inv)\n",
    "        return out\n",
    "\n",
    "\n",
    "summary_net = BottleneckSummaryNet(inv_meta=summary_meta, \n",
    "                                   n_out=(sum(range(1, D+1)) + D) * 2,   # twice the required\n",
    "                                   activation_out=None  # linear\n",
    ")\n",
    "\n",
    "\n",
    "inference_meta = {\n",
    "    'n_coupling_layers': 2,\n",
    "    's_args': {\n",
    "        'units': [64, 64, 64],\n",
    "        'activation': 'elu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "    },\n",
    "    't_args': {\n",
    "        'units': [64, 64, 64],\n",
    "        'activation': 'elu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "    },\n",
    "    'n_params': sum(range(1, D+1)) + D,   # lower diagonal cov (1+2+...+D) and D means\n",
    "    'alpha': 1.9,\n",
    "    'permute': True\n",
    "}\n",
    "\n",
    "inference_net = InvertibleNetwork(inference_meta)\n",
    "\n",
    "amortizer = SingleModelAmortizer(inference_net, summary_net)\n",
    "\n",
    "trainer = ParameterEstimationTrainer(amortizer,\n",
    "                      generative_model,\n",
    "                      loss=mmd_kl_loss,\n",
    "                      learning_rate=0.0001,\n",
    "                      checkpoint_path='export_ckpt/stefan/full_cov_5D',\n",
    "                      max_to_keep = 2\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = trainer.train_rounds(epochs=2, rounds=2, sim_per_round=10000, batch_size=1000, n_obs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_list_to_mean_cov(theta, D):\n",
    "    # setup\n",
    "    n_sim = theta.shape[0]\n",
    "    mean = theta[:, :D]\n",
    "    cov_list = theta[:, D:]\n",
    "    tril_idx1, tril_idx2 = np.tril_indices(D)\n",
    "    cov = np.zeros((n_sim, D, D))\n",
    "    \n",
    "    # fill cov matrix\n",
    "    for i in range(n_sim):\n",
    "        # write from flattened array into in lower triangular matrix\n",
    "        cov[i, tril_idx1, tril_idx2] = cov_list[i, :]\n",
    "        \n",
    "        # mirror lower triangular matrix to upper triangle\n",
    "        cov[i] = cov[i] + cov[i].T - np.diag(np.diag(cov[i]))\n",
    "    return mean, cov\n",
    "\n",
    "def cov_to_corr(cov, std_devs_on_diagonal=True, epsilon=1e-6):\n",
    "    corr = np.zeros_like(cov)\n",
    "    n_sim = cov.shape[0]\n",
    "    \n",
    "    for i in range(n_sim):\n",
    "        # extract 2D matrix\n",
    "        Sigma = cov[i, :, :]\n",
    "        \n",
    "        # transform 2D cov matrix into corr matrix\n",
    "        std_devs = np.sqrt(np.maximum(np.diag(Sigma), epsilon))\n",
    "        Dinv = np.diag(1 / std_devs)\n",
    "        corr[i] = Dinv @ Sigma @ Dinv\n",
    "        \n",
    "        # increase information by putting SDs on diagonal instead of 1's\n",
    "        if std_devs_on_diagonal:\n",
    "            np.fill_diagonal(corr[i], std_devs)\n",
    "            \n",
    "    return corr\n",
    "\n",
    "def theta_cov_to_corr(theta, D):\n",
    "    mean, cov = param_list_to_mean_cov(theta, D=D)\n",
    "    corr = cov_to_corr(cov)\n",
    "    theta_corr = param_transform_full_cov((mean, corr))\n",
    "    return theta_corr\n",
    "\n",
    "def true_vs_estimated_tril(theta_true, theta_est, param_names, D, dpi=300,\n",
    "                      figsize=(20, 4), show=True, filename=None, font_size=12):\n",
    "    \"\"\" Plots a scatter plot with abline of the estimated posterior means vs true values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta_true: np.array\n",
    "        Array of true parameters.\n",
    "    theta_est: np.array\n",
    "        Array of estimated parameters.\n",
    "    param_names: list\n",
    "        List of parameter names for plotting.\n",
    "    D : int\n",
    "        Number of dimensions of parameters.\n",
    "    dpi: int, default:300\n",
    "        Dots per inch (dpi) for the plot.\n",
    "    figsize: tuple(int, int), default: (20,4)\n",
    "        Figure size.\n",
    "    show: boolean, default: True\n",
    "        Controls if the plot will be shown\n",
    "    filename: str, default: None\n",
    "        Filename if plot shall be saved\n",
    "    font_size: int, default: 12\n",
    "        Font size\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    idx = 0\n",
    "\n",
    "    # Plot settings\n",
    "    plt.rcParams['font.size'] = font_size\n",
    "\n",
    "    # Determine n_subplots dynamically\n",
    "    n_row = D+1\n",
    "    n_col = D\n",
    "\n",
    "    # Initialize figure\n",
    "    f, axarr = plt.subplots(n_row, n_col, figsize=figsize)\n",
    "        \n",
    "    # --- Plot true vs estimated posterior means on a single row --- #\n",
    "    \n",
    "    for i in range(n_row):\n",
    "        for j in range(n_col):\n",
    "            if j>(i-1) and i != 0:\n",
    "                axarr[i, j].axis('off')\n",
    "            else:\n",
    "                # Plot analytic vs estimated\n",
    "                axarr[i, j].scatter(theta_est[:, idx], theta_true[:, idx], color='black', alpha=0.4)\n",
    "\n",
    "                # get axis limits and set equal x and y limits\n",
    "                lower_lim = min(axarr[i, j].get_xlim()[0], axarr[i, j].get_ylim()[0])\n",
    "                upper_lim = max(axarr[i, j].get_xlim()[1], axarr[i, j].get_ylim()[1])\n",
    "                axarr[i, j].set_xlim((lower_lim, upper_lim))\n",
    "                axarr[i, j].set_ylim((lower_lim, upper_lim))\n",
    "                axarr[i, j].plot(axarr[i, j].get_xlim(), axarr[i, j].get_xlim(), '--', color='black')\n",
    "\n",
    "                # Compute NRMSE\n",
    "                rmse = np.sqrt(np.mean( (theta_est[:, idx] - theta_true[:, idx])**2 ))\n",
    "                nrmse = rmse / (theta_true[:, idx].max() - theta_true[:, idx].min())\n",
    "                axarr[i, j].text(0.1, 0.9, 'NRMSE={:.3f}'.format(nrmse),\n",
    "                             horizontalalignment='left',\n",
    "                             verticalalignment='center',\n",
    "                             transform=axarr[i, j].transAxes,\n",
    "                             size=10)\n",
    "\n",
    "                # Compute R2\n",
    "                #r2 = r2_score(theta_true[:, j], theta_est[:, j])\n",
    "                #axarr[j].text(0.1, 0.8, '$R^2$={:.3f}'.format(r2),\n",
    "                #             horizontalalignment='left',\n",
    "                #             verticalalignment='center',\n",
    "                #             transform=axarr[j].transAxes, \n",
    "                #             size=10)\n",
    "\n",
    "                if j == 0 and i == 0 or 0==0:\n",
    "                    # Label plot\n",
    "                    axarr[i, j].set_xlabel('Estimated')\n",
    "                    axarr[i, j].set_ylabel('True')\n",
    "                axarr[i, j].set_title(param_names[idx])\n",
    "                axarr[i, j].spines['right'].set_visible(False)\n",
    "                axarr[i, j].spines['top'].set_visible(False)\n",
    "                \n",
    "                idx += 1\n",
    "    \n",
    "    # Adjust spaces\n",
    "    f.tight_layout()\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    if filename is not None:\n",
    "        f.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, x = generative_model(200, 100)\n",
    "param_samples = trainer.network.sample(x, n_samples=200)\n",
    "param_means = param_samples.mean(axis=0)\n",
    "\n",
    "theta = theta_cov_to_corr(theta, D=D)\n",
    "param_means = theta_cov_to_corr(param_means, D=D)\n",
    "\n",
    "param_names = ['mu_{}'.format(i) for i in range(1, D+1)] + \\\n",
    "['sd_{}'.format(i+1) if i==j else 'cor_{}{}'.format(i+1, j+1) for (i, j) in zip(*np.tril_indices(D))]\n",
    "\n",
    "# true parameters\n",
    "print(\"BayesFlow (x) vs. true thetas (y) -- Recovery of true thetas\")\n",
    "true_vs_estimated_tril(theta, param_means, param_names, D, figsize=(20,20))\n",
    "\n",
    "#print('\\n\\nSummary network response for one batch')\n",
    "#s = np.array(trainer.network.summary_net(x))\n",
    "#sns.pairplot(pd.DataFrame(s, columns=['s_{}'.format(i) for i in range(1, s.shape[1]+1)]), kind=\"kde\")\n",
    "\n",
    "#print('\\n\\nFull posterior for one dataset.')\n",
    "#p = param_samples[0]\n",
    "#sns.pairplot(pd.DataFrame(p, columns=['dim_{}'.format(i) for i in range(1, p.shape[1]+1)]), kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytic posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytic_joint_posterior_normal_inverse_wishart(X, mu_0, lamda_0, Psi_0, nu_0):\n",
    "    n_sim, n_obs, D = X.shape\n",
    "    mu_n = [None] * n_sim\n",
    "    lamda_n = [None] * n_sim\n",
    "    Psi_n = [None] * n_sim\n",
    "    nu_n = [None] * n_sim\n",
    "    \n",
    "    for i in range(n_sim):\n",
    "        x = X[i, :, :]\n",
    "        x_bar = np.mean(x, axis=0)\n",
    "        C = np.dot((x-x_bar).T, x-x_bar)\n",
    "        \n",
    "        mu_n[i] = (lamda_0 * mu_0 + n_obs*x_bar) / (lamda_0 + n_obs)\n",
    "        lamda_n[i] = lamda_0 + n_obs\n",
    "        nu_n[i] = nu_0 + n_obs\n",
    "        Psi_n[i] = Psi_0 + C + ((lamda_0*n_obs)/(lamda_0+n_obs)) * np.dot(x_bar-mu_0, (x_bar-mu_0).T)\n",
    "        \n",
    "    return mu_n, lamda_n, Psi_n, nu_n\n",
    "\n",
    "\n",
    "def marginal_posterior_normal_inverse_wishart(mu_n, lamda_n, Psi_n, nu_n):\n",
    "    D = mu_n[0].shape[0]\n",
    "    n_sim = len(mu_n)\n",
    "    \n",
    "    marginal_mu_distributions = [None] * n_sim\n",
    "    marginal_Sigma_distributions = [None] * n_sim\n",
    "\n",
    "    for i in range(n_sim):\n",
    "        # mu_p\n",
    "        marginal_mu_distributions[i] = stats.multivariate_t(\n",
    "            loc=mu_n[i], \n",
    "            shape=np.linalg.inv(Psi_n[i]) / (lamda_n[i]*(nu_n[i] - D + 1)), \n",
    "            df=nu_n[i] - D + 1\n",
    "        )\n",
    "        \n",
    "        # Sigma_p\n",
    "        marginal_Sigma_distributions[i] = stats.invwishart(nu_n[i], Psi_n[i])\n",
    "    \n",
    "    return marginal_mu_distributions, marginal_Sigma_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, x = generative_model(200, 100)\n",
    "param_samples = trainer.network.sample(x, n_samples=200)\n",
    "param_means = param_samples.mean(axis=0)\n",
    "\n",
    "param_means = theta_cov_to_corr(param_means, D=D)\n",
    "\n",
    "\n",
    "mu_n, lamda_n, Psi_n, nu_n = analytic_joint_posterior_normal_inverse_wishart(x, mu_0, lamda_0, Psi_0, nu_0)\n",
    "\n",
    "marginal_mu_p, marginal_Sigma_p = marginal_posterior_normal_inverse_wishart(mu_n, lamda_n, Psi_n, nu_n)\n",
    "\n",
    "mu_p_means = np.array([dist.loc for dist in marginal_mu_p])\n",
    "Sigma_p_covs = np.array([dist.scale for dist in marginal_Sigma_p])\n",
    "\n",
    "theta_analytical_posterior = param_transform_full_cov((mu_p_means, Sigma_p_covs))\n",
    "theta_analytical_posterior = theta_cov_to_corr(theta_analytical_posterior, D=D)\n",
    "\n",
    "param_names = ['mu_{}'.format(i) for i in range(1, D+1)] + \\\n",
    "['sd_{}'.format(i+1) if i==j else 'cor_{}{}'.format(i+1, j+1) for (i, j) in zip(*np.tril_indices(D))]\n",
    "\n",
    "# true parameters\n",
    "print(\"BayesFlow (x) vs. true thetas (y) -- Recovery of analytical posteriors\")\n",
    "true_vs_estimated_tril(theta_analytical_posterior, param_means, param_names, D, figsize=(20,20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
